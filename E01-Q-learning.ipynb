{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(gym.make(\"CartPole-v0\"), \"./video\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "We are trying to learn the state-action value function, $Q(s, a)$ using **vanilla Q-learning**.\n",
    "\n",
    "Under Bellman's optimality condition, if we were functioning under some optimal policy, $\\pi^*$, the regardless of which state we are in, our actions, $a=\\pi^*(s)$, would lead us to the maximum expected value. This means that the equation below holds.\n",
    "\n",
    "$$\n",
    "Q_{\\pi^*} (s, a) = r + \\gamma \\underset{a}\\max Q_{\\pi^*}(s^\\prime, \\pi^*(s^\\prime))\n",
    "$$\n",
    "\n",
    "The above can thus be an update equation. Suppose our policy is now to choose the action that maximises our expected value given our current state (i.e. $\\pi(s) = \\underset{a}{\\text{argmax}} Q(s, a)$), our aim would be to minimise the difference between the left and right side of the above equation.\n",
    "\n",
    "Our cost metric, also known as temporal difference,  is thus\n",
    "\n",
    "$$\n",
    "\\delta = Q(s, a) - \\left(r + \\gamma \\underset{a}\\max Q(s^\\prime, \\pi^*(s^\\prime))\\right)\n",
    "$$\n",
    "\n",
    "Our overall simulation method will be that of **continuous every visit Monte Carlo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs,\n",
    "                 hidden_layer_sizes=[16, 32, 16],\n",
    "                 max_memory=10000):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        layer_sizes = [n_inputs] + hidden_layer_sizes + [n_outputs]\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        for s_in, s_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            \n",
    "            self.layers.append(nn.Linear(s_in, s_out))\n",
    "            \n",
    "        self.params = []\n",
    "        \n",
    "        for l in self.layers:\n",
    "            \n",
    "            self.params.extend(l.parameters())\n",
    "            \n",
    "        self.params = nn.ParameterList(self.params)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for l in self.layers:\n",
    "            \n",
    "            x = F.relu(l(x))\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, max_memory, batch_size):\n",
    "        \n",
    "        self.memory = []\n",
    "        self.max_memory = max_memory\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def remember(self, cur_state, next_state, action, reward, is_done):\n",
    "        \n",
    "        self.memory.append((cur_state, next_state, action, reward, is_done))\n",
    "            \n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            \n",
    "            self.memory = self.memory[-self.max_memory:]\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def sample(self):\n",
    "        \n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.memory)\n",
    "    \n",
    "def to_torch(x, device):\n",
    "    \n",
    "    return torch.tensor(x, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = env.observation_space.shape[0]\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "episode_length = 200\n",
    "max_episodes = 5000\n",
    "\n",
    "memory_capacity = 5000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "episode_score_history = []\n",
    "\n",
    "epsilon_max = 0.99\n",
    "epsilon_decay = 0.9999\n",
    "epsilon_min = 0.05\n",
    "\n",
    "gamma = 0.99\n",
    "best_loss = np.inf\n",
    "best_model = None\n",
    "min_cost_improvement = 1e-3\n",
    "\n",
    "report_every = 50\n",
    "score_threshold = 195\n",
    "\n",
    "output_dir = \"E01-model\"\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    \n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = QNet(n_inputs, n_outputs, hidden_layer_sizes=[8, 8]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(memory_capacity, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = epsilon_max\n",
    "optimizer = optim.RMSprop(net.parameters())\n",
    "criteria = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binghao/anaconda3/envs/torch/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Tried to pass invalid video frame, marking as broken: Your frame has shape (1045, 634, 3), but the VideoRecorder is configured for shape (1045, 632, 3).\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50 done. Loss: 1.0. Ave. episode score (last 50): 21.48. Last epsilon: 0.8892\n",
      "Episode: 100 done. Loss: 124028.5078. Ave. episode score (last 50): 20.11. Last epsilon: 0.8096\n",
      "Episode: 150 done. Loss: 2768.8. Ave. episode score (last 50): 18.25. Last epsilon: 0.7408\n",
      "Episode: 200 done. Loss: 1.4518. Ave. episode score (last 50): 20.85. Last epsilon: 0.6573\n",
      "Episode: 250 done. Loss: 63.6109. Ave. episode score (last 50): 25.61. Last epsilon: 0.5735\n",
      "Episode: 300 done. Loss: 30.5557. Ave. episode score (last 50): 21.45. Last epsilon: 0.5304\n",
      "Episode: 350 done. Loss: 34.9268. Ave. episode score (last 50): 14.8. Last epsilon: 0.4946\n",
      "Episode: 400 done. Loss: 39.7656. Ave. episode score (last 50): 13.38. Last epsilon: 0.4639\n",
      "Episode: 450 done. Loss: 40.8212. Ave. episode score (last 50): 13.24. Last epsilon: 0.4332\n",
      "Episode: 500 done. Loss: 31.9179. Ave. episode score (last 50): 13.03. Last epsilon: 0.4073\n",
      "Episode: 550 done. Loss: 22.3551. Ave. episode score (last 50): 12.23. Last epsilon: 0.3834\n",
      "Episode: 600 done. Loss: 19.2319. Ave. episode score (last 50): 12.12. Last epsilon: 0.3608\n",
      "Episode: 650 done. Loss: 5.0872. Ave. episode score (last 50): 11.71. Last epsilon: 0.341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97953898fd9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         memory.remember(cur_state,\n\u001b[1;32m     30\u001b[0m                             \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                             \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                             \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                             is_done)\n",
      "\u001b[0;32m<ipython-input-9-2c1bb4ef9f02>\u001b[0m in \u001b[0;36mto_torch\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(max_episodes):\n",
    "    \n",
    "    cur_state = torch.from_numpy(env.reset()).type(torch.float32).to(device)\n",
    "    \n",
    "    episode_score = 0\n",
    "    \n",
    "    for i in count():\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        state_value = net(cur_state)\n",
    "        \n",
    "        if np.random.rand() <= epsilon:\n",
    "            \n",
    "            action = np.random.randint(n_outputs)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "            \n",
    "                action = state_value.argmax().item()\n",
    "            \n",
    "        epsilon = np.max([epsilon * epsilon_decay, epsilon_min])\n",
    "        \n",
    "        next_state, reward, is_done, info = env.step(action)\n",
    "        \n",
    "        episode_score += reward\n",
    "        \n",
    "        memory.remember(cur_state,\n",
    "                            to_torch(next_state, device),\n",
    "                            to_torch([action], device),\n",
    "                            to_torch([reward], device),\n",
    "                            is_done)\n",
    "        \n",
    "        if is_done:\n",
    "            \n",
    "            episode_score_history.append(episode_score)\n",
    "            \n",
    "            break\n",
    "              \n",
    "        cur_state = to_torch(next_state, device)\n",
    "            \n",
    "        if len(memory) >= 3 * batch_size:\n",
    "            \n",
    "            cur_states_, next_states_, actions_, rewards_, is_done_ = zip(*memory.sample())\n",
    "            \n",
    "            action_batch = torch.cat(actions_).type(torch.long).view(-1, 1)\n",
    "            \n",
    "            state_value_batch = net(torch.cat(cur_states_).view(-1, n_inputs)).gather(1, action_batch)\n",
    "            \n",
    "            reward_batch = torch.cat(rewards_).view(-1, 1)\n",
    "            \n",
    "            is_done_batch = torch.tensor(is_done_, dtype=torch.bool, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                next_state_value_batch = net(torch.cat(next_states_).view(-1, n_inputs)).max(1, keepdim=True).values.detach()\n",
    "                \n",
    "                next_state_value_batch[is_done_batch] = 0\n",
    "            \n",
    "            loss = criteria(state_value_batch,\n",
    "                            reward_batch + (gamma * next_state_value_batch))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "    if np.mean(episode_score_history[-100:]) >= score_threshold:\n",
    "        \n",
    "        print(\"Game solved!\")\n",
    "        \n",
    "        break\n",
    "    \n",
    "    if (e + 1) % report_every == 0:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            print(\"Episode: {} done. Loss: {}. Ave. episode score (last {}): {}. Last epsilon: {}\"\n",
    "                  .format(e + 1,\n",
    "                          np.round(loss.item(), 4),\n",
    "                          report_every,\n",
    "                          np.mean(episode_score_history[-100:]),\n",
    "                          np.round(epsilon, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.Series(episode_score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.plot()\n",
    "scores.rolling(100).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.rolling(100).mean().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), os.path.join(output_dir, \"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_episodes = 100\n",
    "eval_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(n_eval_episodes):\n",
    "    \n",
    "    cur_state = to_torch(env.reset(), device)\n",
    "\n",
    "    for i in range(200):\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        next_state, reward, is_done, info = env.step(net(cur_state).argmax().item())\n",
    "\n",
    "        cur_state = to_torch(next_state, device)\n",
    "\n",
    "        if is_done:\n",
    "\n",
    "            break\n",
    "    \n",
    "    if e % 10 == 9:\n",
    "        \n",
    "        print(\"Evaluated {} episodes\".format(e))\n",
    "            \n",
    "    eval_scores.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(eval_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
